<!doctype html>
<html>

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
    <!-- Open Graph Meta Tags -->
    <meta property="og:title" content="Does Thinking More Always Help? Understanding Test-Time Scaling in Reasoning Models" />
    <meta property="og:description" content="We investigate whether extended thinking at test-time truly improves reasoning in AI models. Our research reveals the illusion of extended thinking and introduces parallel thinking as an effective alternative, achieving up to 20% higher accuracy." />
    <meta property="og:image" content="https://avinashreddydev.github.io/scaling-mirage/og.jpeg" />
    <meta property="og:image:alt" content="Comparison of test-time scaling strategies showing parallel thinking vs extended thinking approaches" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />
    <meta property="og:url" content="https://avinashreddydev.github.io/scaling-mirage/" />
    <meta property="og:type" content="article" />
    <meta property="og:site_name" content="Test-Time Scaling Research" />
    <meta property="og:locale" content="en_US" />
    
    <!-- LinkedIn-specific Open Graph Tags -->
    <meta property="article:author" content="Soumya Surva Ghosal" />
    <meta property="article:author" content="Souradip Chakraborthy" />
    <meta property="article:author" content="Avinash Reddy" />
    <meta property="article:published_time" content="2025-01-01T00:00:00Z" />
    <meta property="article:section" content="Artificial Intelligence" />
    <meta property="article:tag" content="AI Research" />
    <meta property="article:tag" content="Reasoning Models" />
    <meta property="article:tag" content="Test-Time Scaling" />
    <meta property="article:tag" content="Machine Learning" />
    
    <!-- WhatsApp-optimized Open Graph Tags -->
    <meta property="og:image:secure_url" content="https://avinashreddydev.github.io/scaling-mirage/og.jpeg" />
    <meta property="og:video" content="" />
    <meta property="og:audio" content="" />
    
    <!-- LinkedIn Company/Organization -->
    <meta property="og:see_also" content="https://saferrai-lab.org/" />
    <meta property="og:see_also" content="https://www.umd.edu/" />
    <meta property="og:see_also" content="https://www.ucf.edu/" />
    
    <!-- Twitter Card Meta Tags -->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Does Thinking More Always Help? Understanding Test-Time Scaling in Reasoning Models" />
    <meta name="twitter:description" content="Research on test-time scaling in reasoning models, revealing the illusion of extended thinking and introducing parallel thinking for better performance." />
    <meta name="twitter:image" content="https://avinashreddydev.github.io/scaling-mirage/og.jpeg" />
    <meta name="twitter:image:alt" content="Comparison of test-time scaling strategies" />
    
    <!-- Additional Meta Tags -->
    <meta name="description" content="Academic research on test-time scaling in reasoning models, investigating whether thinking more at test-time truly leads to better reasoning performance." />
    <meta name="keywords" content="AI, reasoning models, test-time scaling, parallel thinking, machine learning, artificial intelligence, DeepSeek, OpenAI" />
    <meta name="author" content="Soumya Surva Ghosal, Souradip Chakraborthy, Avinash Reddy, et al." />
    
    <script src="https://cdn.jsdelivr.net/npm/@tailwindcss/browser@4"></script>


    <style type="text/tailwindcss">
        @theme {
          --color-primary: #363636;
        }
      </style>
    <title>Does Thinking More Always Help? Understanding Test-Time Scaling in Reasoning Models</title>
</head>

<body class="bg-gray-50 min-h-screen">
    <div class="max-w-6xl mx-auto bg-white shadow-lg">
        <!-- Header -->
        <div class="p-8">
            <div class="text-center">
                <h1 class="text-5xl text-primary font-normal mb-8 leading-tight">
                    Does Thinking More <em>Always</em> Help?<br>
                    <span class="">Understanding Test-Time Scaling in Reasoning Models</span>
                </h1>
                
                <!-- Venue Badge
                <div class="mb-8">
                    <span class="  px-6 py-2 rounded-lg font-semibold text-lg">
                        Under Submission 2025
                    </span>
                </div> -->
                
                <!-- Authors Section -->
                <div class="mb-8 text-lg leading-relaxed">
                    <p class=" mb-4">
                        <span class="font-normal text-blue-400">Soumya Surva Ghosal</span><sup class="text-xs">1</sup>, 
                        <span class="font-normal text-blue-400">Souradip Chakraborthy</span><sup class="text-xs">1</sup>, 
                        <span class="font-normal text-blue-400">Avinash Reddy</span><sup class="text-xs">2</sup>, 
                        <span class="font-normal text-blue-400">Yifu Li</span><sup class="text-xs">3</sup>,<br>
                        <span class="font-normal text-blue-400">Mengi Wang</span><sup class="text-xs">4</sup>, 
                        <span class="font-normal text-blue-400">Dinesh Manocha</span><sup class="text-xs">1</sup>, 
                        <span class="font-normal text-blue-400">Furong Huang</span><sup class="text-xs">1</sup>,<br>
                        <span class="font-normal text-blue-400">Mohammad Ghavamzadeh</span><sup class="text-xs">5</sup>, 
                        <span class="font-normal text-blue-400">Amrit Singh Bedi</span><sup class="text-xs">2</sup>
                    </p>
                </div>

                <!-- Institutions -->
                <div class=" text-base mb-6 leading-relaxed">
                    <p>
                        <sup class="text-xs">1</sup><a href="https://www.umd.edu/" >University of Maryland</a> &nbsp;&nbsp;
                        <sup class="text-xs">2</sup><a href="https://www.ucf.edu/" >University of Central Florida</a> &nbsp;&nbsp;
                        <sup class="text-xs">3</sup><a href="https://umich.edu/" >University of Michigan</a>
                    </p>
                    <p>
                        <sup class="text-xs">4</sup><a href="https://www.princeton.edu/" class="">Princeton University</a> &nbsp;&nbsp;
                        <sup class="text-xs">5</sup><a href="https://www.amazon.science/" class="">Amazon AGI</a>
                    </p>
                </div>

                <!-- Contribution Notes -->
                <!-- <div class=" text-sm mb-6 italic">
                    <p>* denotes equal contribution</p>
                    <p>† denotes equal advising</p>
                </div> -->

                <!-- Navigation Buttons -->
                <div class="flex flex-wrap justify-center gap-4 mt-6">
                    <a href="#"
                        class="bg-gray-800 text-white  px-6 py-2 rounded-full  font-semibold    transition-colors">
                        📄 Paper
                    </a>
                    <a href="#"
                        class="bg-gray-800 text-white  px-6 py-2 rounded-full  font-semibold transition-colors">
                        💾 Supplementary
                    </a>
                    <a href="#"
                        class="bg-gray-800 text-white  px-6 py-2 rounded-full  font-semibold transition-colors">
                        💻 Code
                    </a>
                    <a href="#"
                        class="bg-gray-800 text-white  px-6 py-2 rounded-full  font-semibold transition-colors">
                        📊 arXiv
                    </a>
                </div>
            </div>
        </div>

        <!-- Abstract Section -->
        <div class="p-8 border-b border-gray-200 flex flex-col items-center justify-center max-w-4xl mx-auto">
            

            



            <div class="max-w-4xl mx-auto text-justify leading-relaxed text-gray-700 space-y-4 text-lg">

                <h1 class="text-3xl font-semibold mb-6 text-center text-primary underline">Abstract</h1>
                <p>
                    Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1, DeepSeek R1) have led to a
                    popular belief that extending thinking traces using prompts like "Wait" or "Let me rethink" can
                    improve performance. This raises a natural question: <em>Does thinking more at test-time truly lead
                        to better reasoning?</em>
                </p>
                <p>
                    To answer this question, we perform a detailed empirical study across models and benchmarks, which
                    reveals a consistent pattern of initial performance improvements from additional thinking followed
                    by a decline, due to <em>overthinking</em>. To understand this non-monotonic trend, we consider a
                    simple probabilistic model, which reveals that additional thinking increases output
                    variance—creating an illusion of improved reasoning while ultimately undermining precision.
                </p>
                <p>
                    Thus, observed gains from "more thinking" are not true indicators of improved reasoning, but
                    artifacts stemming from the connection between model uncertainty and evaluation metric. This
                    suggests that test-time scaling through extended thinking is not an effective way to utilize the
                    inference thinking budget.
                </p>
                <p>
                    Recognizing these limitations, we introduce an alternative test-time scaling approach, <em>parallel
                        thinking</em>, inspired by Best-of-N sampling. Our method generates multiple independent
                    reasoning paths within the same inference budget and selects the most consistent response via
                    majority vote, achieving up to <strong>20%</strong> higher accuracy compared to extended thinking.
                    This provides a simple yet effective mechanism for test-time scaling of reasoning models.
                </p>
            </div>
        </div>

        <!-- Main Results Figure -->

        <div class="flex flex-col items-center justify-center max-w-4xl mx-auto">
            <div class="p-8 border-b border-gray-200 block">
                <!-- <h2 class="text-3xl font-bold mb-6 text-center text-gray-800">Key Findings</h2> -->
                <h2 class="text-3xl font-semibold mb-6 text-center text-primary">
                    Does thinking more at test-time truly lead to better reasoning?
                </h2>
                
                <img src="https://avinashreddydev.github.io/scaling-mirage/key-findings.png" alt="Main Results"
                    class="w-full h-auto">
                <p> <b>Figure 1:</b> Results for Wait & Think more. To investigate the variation in accuracy with
                    increasing
                    average
                    thinking tokens generated during inference, we plot accuracy as a function of thinking tokens across
                    three
                    datasets: GSM-8K [ 5], MATH-500 [ 20], and AIME [ 15 ]. The average has been taken over all prompts
                    in
                    the
                    test dataset. We report results for three SoTA open-source reasoning models:
                    DeepSeek-R1-Distill-Qwen-1.5B
                    (1st row), DeepSeek-R1-Distill-Qwen-7B (2nd row), and DeepSeek-R1-Distill-Llama-8B model (3rd row)
                    [8].
                    During test-time, following Muennighoff et al. [22], we induce the model to generate more thinking
                    tokens by
                    suppressing the end-of-thinking token delimiter (</think>) and instead appending "Wait" to the
                    model's
                    current thinking trace to encourage additional thinking. The baseline with no budget control
                    (standard
                    thinking)
                    is marked by ⋆. Successive salmon circles to the right of the star correspond to one, two, . . .
                    additional "Wait"
                    insertions to the model's thinking trace. From our experiments, we note that accuracy initially
                    rises as
                    the model
                    is prompted to think longer, but then gradually falls off for all settings.</p>
            </div>


            <div class="p-8 border-b border-gray-400 block">
                <h2 class="text-3xl font-semibold mb-6 text-center text-primary">Proposed Method : Parallel Thinking</h2>
                <img src="https://avinashreddydev.github.io/scaling-mirage/bon.png" alt="Main Results"
                    class="w-full h-auto">
                <p> <b>Figure 2:</b> Comparing test-time scaling strategies. We compare test-time scaling using "Wait &
                    Think more"
                    and parallel thinking across three benchmark datasets (GSM-8K [5], MATH-500 [ 10], and AIME 2024 [15
                    ])
                    and three state-of-the-art reasoning models: DeepSeek-R1-Distill-Qwen-1.5B (first row),
                    DeepSeek-R1-Distill-
                    Qwen-7B (second row), and DeepSeek-R1-Distill-Llama-8B (third row). Each plot shows average accuracy
                    (y-axis) against average thinking tokens (x-axis) as the thinking budget is progressively increased,
                    with averages
                    taken across all test set prompts. Baseline performance without budget control (standard thinking)
                    is
                    marked by
                    a black star. Additional thinking using "Wait & Think More" is shown as salmon circles, while
                    parallel
                    scaling
                    is represented by green circles. Unlike sequential scaling, which can degrade performance with
                    overthinking,
                    parallel scaling consistently maintains or improves accuracy as the budget increases.
                </p>
            </div>


            <div class="p-8 border-b border-gray-400 block max-w-4xl mx-auto flex flex-col items-center justify-center" >
                <img src="https://avinashreddydev.github.io/scaling-mirage/budget_new.png" alt="Abstract"
                    class="w-1/2 h-auto my-4">
                <p class="text-center  mt-4"> <b>Figure 3:</b>
                    Effective utilization of test-time budget: Given a fixed
                    inference budget of 16K tokens, parallel thinking results in around
                    22% and 47% more accuracy compared to Wait & Think more and
                    Exact thinking TTBC respectively.
                </p>
            </div>


        </div>


        <!-- <p class="text-center text-gray-600 mt-4 italic">
            Comparison of extended thinking vs. parallel thinking approaches across different reasoning benchmarks.
        </p> -->
    </div>

   

    <!-- BibTeX Section -->
    <div class="p-8 bg-gray-50">
        <h2 class="text-3xl font-semibold mb-6 text-center text-primary">BibTeX</h2>
        <div class="bg-gray-100 p-6 rounded-lg border max-w-4xl mx-auto">
            <pre class="text-sm text-gray-700 overflow-x-auto"><code>@article{ghosal2025testtime,
  title={Test-Time Scaling in Reasoning Models: The Illusion of Extended Thinking},
  author={Ghosal, Soumya Surva and Chakraborthy, Souradip and Reddy, Avinash and Li, Yifu and Wang, Mengi and Manocha, Dinesh and Huang, Furong and Ghavamzadeh, Mohammad and Bedi, Amrit Singh},
  journal={Under Submission},
  year={2025}
}</code></pre>
        </div>
    </div>

    <!-- Footer -->
    <div class="bg-gray-800 text-white">
        <div class="max-w-6xl mx-auto px-6 py-8">
            <!-- Main Footer Content -->
            <div class="grid grid-cols-1 md:grid-cols-3 gap-8 mb-6">
                <!-- Research Group Info -->
                <div class="text-center md:text-left">
                    <h3 class="text-lg font-semibold mb-3 text-white">Research Group</h3>
                    <p class="text-gray-300 text-sm mb-2">SaferrAI Lab</p>
                    <a href="https://saferrai-lab.org/" class="text-white hover:text-gray-300 text-sm transition-colors">
                        🌐 saferrai-lab.org
                    </a>
                </div>
                
                <!-- Paper Info -->
                <div class="text-center">
                    <h3 class="text-lg font-semibold mb-3 text-white">Publication</h3>
                    <!-- <p class="text-gray-300 text-sm mb-2">Under Submission 2025</p> -->
                    <p class="text-gray-400 text-xs">Test-Time Scaling in Reasoning Models</p>
                </div>
                
                <!-- Links -->
                <div class="text-center md:text-right">
                    <h3 class="text-lg font-semibold mb-3 text-white">Resources</h3>
                    <div class="space-y-2">
                        <a href="#" class="block text-white hover:text-gray-300 text-sm transition-colors">📄 Paper</a>
                        <a href="#" class="block text-white hover:text-gray-300 text-sm transition-colors">💻 Code</a>
                        <a href="#" class="block text-white hover:text-gray-300 text-sm transition-colors">📊 arXiv</a>
                    </div>
                </div>
            </div>
            
            <!-- Divider -->
            <div class="border-t border-gray-600 mb-6"></div>
            
            <!-- Bottom Footer -->
            <div class="flex flex-col md:flex-row justify-between items-center space-y-4 md:space-y-0">
                <!-- Copyright -->
                <div class="text-center md:text-left">
                    <p class="text-gray-400 text-sm">
                        © 2025 SaferrAI Lab. All rights reserved.
                    </p>
                </div>
                
                <!-- Template Attribution -->
                
            </div>
            
            <!-- Decorative Element -->
            <div class="mt-6 flex justify-center">
                <div class="w-24 h-1 bg-white rounded-full"></div>
            </div>
        </div>
    </div>
</body>

</html>
